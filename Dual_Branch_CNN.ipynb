{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5067fa7-0c15-4276-a6f8-d598aaba7f6c",
      "metadata": {
        "id": "a5067fa7-0c15-4276-a6f8-d598aaba7f6c"
      },
      "outputs": [],
      "source": [
        "## IMPORTING OF LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import scipy.io as sio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import os\n",
        "import random\n",
        "from random import shuffle\n",
        "import scipy.ndimage\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Input, Concatenate\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7dbcc9-f13e-47ae-940f-0a59a4ac1700",
      "metadata": {
        "id": "9c7dbcc9-f13e-47ae-940f-0a59a4ac1700"
      },
      "outputs": [],
      "source": [
        "#this function is used to load all data i.e complete labelled samples\n",
        "def loadData(path, filename):\n",
        "    ''' x contains the features in image dimension and labels contains the actual gt'''\n",
        "    data_path = os.path.join(os.getcwd(), path)\n",
        "    data = sio.loadmat(os.path.join(data_path, filename))['data']\n",
        "    y = sio.loadmat(os.path.join(data_path, 'gt.mat'))['label']\n",
        "    x = data\n",
        "    labels = y\n",
        "    return x, labels\n",
        "\n",
        "\n",
        "# filename = 'T3_SVD'\n",
        "filename = 'RF_svd'\n",
        "\n",
        "# data_path = os.path.join(os.getcwd(),\"/home/workspace/RS2/\")\n",
        "# data_path = os.path.join(os.getcwd(),\"/home/workspace/airsar-sf/\")\n",
        "data_path = os.path.join(os.getcwd(),\"/home/workspace/flevo/\")\n",
        "\n",
        "X, labels = loadData(data_path, filename)\n",
        "X.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7d0ffb-3bc7-46eb-83d2-688ede766c25",
      "metadata": {
        "id": "ea7d0ffb-3bc7-46eb-83d2-688ede766c25"
      },
      "outputs": [],
      "source": [
        "\n",
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "\n",
        "def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "\n",
        "windowSize = 7\n",
        "XPatches, yPatches = createPatches(X, labels, windowSize=windowSize)\n",
        "\n",
        "XPatches.shape, yPatches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da82453",
      "metadata": {
        "id": "4da82453"
      },
      "outputs": [],
      "source": [
        "def reports(model, X_test, y_test):\n",
        "    Y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# #     ## target_names for Flevo\n",
        "    target_names = ['a', 'b', 'c', 'd'\n",
        "            ,'e', 'f', 'g',\n",
        "                'h', 'i', 'j', 'k',  'l', 'm', 'n', 'o']\n",
        "\n",
        "    ## target_names for SF\n",
        "    # target_names = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "    #\n",
        "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
        "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=256)\n",
        "    print(score)\n",
        "\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "\n",
        "    return classification, confusion, Test_Loss, Test_accuracy\n",
        "\n",
        "def generate_reports(XPatches, yPatches, model, filename, index):\n",
        "    #creation of test patches to include all the samples\n",
        "#     Xp1 = XPatches[:, :, :,:9] ## FOR T3_SVD\n",
        "    Xp1 = XPatches[:, :, :,:6] ## FOR RF_SVD\n",
        "    # Reshape Xpatches2\n",
        "#     Xp2 = XPatches[:, :, :, 9:45] ## FOR T3_SVD\n",
        "    Xp2 = XPatches[:, :, :, 6:30] ## FOR RF_SVD\n",
        "\n",
        "    X_test = [Xp1,Xp2]\n",
        "    y_test = yPatches\n",
        "\n",
        "    #convert the y labels to categorical\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    classification, confusion, Test_loss, Test_accuracy = reports(model,X_test,y_test)\n",
        "\n",
        "    # Calculate class-wise accuracy\n",
        "    class_wise_acc = np.diag(confusion) / np.sum(confusion, axis = 1)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    overall_acc = np.sum(np.diag(confusion)) / np.sum(confusion)\n",
        "\n",
        "    # Calculate average accuracy\n",
        "    average_acc = np.mean(class_wise_acc)\n",
        "\n",
        "    # Calculate chance agreement\n",
        "    chance_agreement = np.sum(np.sum(confusion, axis=1) * np.sum(confusion, axis=0)) / (np.sum(confusion) ** 2)\n",
        "\n",
        "    # Calculate kappa\n",
        "    kappa = (overall_acc - chance_agreement) / (1 - chance_agreement)\n",
        "\n",
        "    print(\"Class-wise accuracy:\", class_wise_acc)\n",
        "    print(\"Overall accuracy:\", overall_acc)\n",
        "    print(\"Average accuracy:\", average_acc)\n",
        "    print(\"Kappa:\", kappa)\n",
        "\n",
        "    classification = str(classification)\n",
        "    confusion = str(confusion)\n",
        "    file_name = f\"Accy of {filename}-{index}.txt\"\n",
        "    with open(file_name, 'w') as x_file:\n",
        "        x_file.write('{} Test loss (%)'.format(Test_loss))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write('\\n')\n",
        "        x_file.write('{}'.format(classification))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write('{}'.format(confusion))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write('\\n')\n",
        "        x_file.write(\"Class-wise accuracy:{}\".format(class_wise_acc))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write(\"Overall accuracy:{}\".format(overall_acc))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write(\"Average accuracy:{}\".format(average_acc))\n",
        "        x_file.write('\\n')\n",
        "        x_file.write(\"Kappa:{}\".format(kappa))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTestSet(X, y, testRatio=0.9):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "u8bnmpqxqCK0"
      },
      "id": "u8bnmpqxqCK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#     *****************Reshape Train & Teset set*****************\n",
        "\n",
        "## Reshape of Train set\n",
        "# X_train1 = X_train[:, :, :,:9] ## FOR T3_SVD\n",
        "X_train1 = X_train[:, :, :,:6] ## FOR RF_SVD\n",
        "# X_train1.shape\n",
        "\n",
        "# X_train2 = X_train[:, :, :, 9:45] ## FOR T3_SVD\n",
        "X_train2 = X_train[:, :, :, 6:30] ## FOR RF_SVD\n",
        "\n",
        "X_train1.shape, X_train2.shape\n",
        "\n",
        "## Reshape of Test set\n",
        "# X_test1 = X_test[:, :, :,:9] ## FOR T3_SVD\n",
        "X_test1 = X_test[:, :, :,:6] ## FOR RF_SVD\n",
        "# X_train1.shape\n",
        "\n",
        "# Reshape Xpatches2\n",
        "# X_test2 = X_test[:, :, :, 9:45] ## FOR T3_SVD\n",
        "X_test2 = X_test[:, :, :, 6:30] ## FOR RF_SVD\n",
        "\n",
        "\n",
        "X_test1.shape, X_test2.shape\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "# Define the input shape\n",
        "input_shape1 = X_train1[0].shape\n",
        "print(\"input_shape1: \",input_shape1)\n",
        "input_shape2 = X_train2[0].shape\n",
        "print(\"input_shape2: \",input_shape2)\n"
      ],
      "metadata": {
        "id": "meOMn-JKqhLJ"
      },
      "id": "meOMn-JKqhLJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#     *************************MODEL*****************\n",
        "num_classes = 15 ## FOR FLEVO\n",
        "# num_classes = 5 ## FOR SF\n",
        "\n",
        "# Create two separate channels\n",
        "input_channel1 = Input(shape=input_shape1)\n",
        "input_channel2 = Input(shape=input_shape2)\n",
        "\n",
        "# Define the shared convolutional layers\n",
        "conv1_channel1 = Conv2D(30, (2, 2), strides=1, padding=\"same\", activation='relu')(input_channel1)\n",
        "maxpool1_channel1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv1_channel1)\n",
        "\n",
        "conv2_channel1 = Conv2D(60, (2, 2), strides=1, padding=\"same\", activation='relu')(maxpool1_channel1)\n",
        "maxpool2_channel1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv2_channel1)\n",
        "\n",
        "conv3_channel1 = Conv2D(120, (2, 2), strides=1, padding=\"same\", activation='relu')(maxpool2_channel1)\n",
        "\n",
        "conv1_channel2 = Conv2D(30, (2, 2), strides=1, padding=\"same\", activation='relu')(input_channel2)\n",
        "maxpool1_channel2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv1_channel2)\n",
        "\n",
        "conv2_channel2 = Conv2D(60, (2, 2), strides=1, padding=\"same\", activation='relu')(maxpool1_channel2)\n",
        "maxpool2_channel2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv2_channel2)\n",
        "\n",
        "conv3_channel2 = Conv2D(120, (2, 2), strides=1, padding=\"same\", activation='relu')(maxpool2_channel2)\n",
        "\n",
        "# Concatenate the outputs of both channels\n",
        "concatenated_channels = Concatenate()([conv3_channel1, conv3_channel2])\n",
        "\n",
        "# Continue with the rest of the model\n",
        "flatten = Flatten()(concatenated_channels)\n",
        "dense1 = Dense(240, activation='relu')(flatten)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(num_classes, activation='softmax')(dropout)\n",
        "\n",
        "# Create the final model with two input channels\n",
        "two_channel_model = Model(inputs=[input_channel1, input_channel2], outputs=output)\n",
        "\n",
        "# Compile the model as needed\n",
        "two_channel_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "two_channel_model.summary()\n",
        "\n",
        "start = time.time()\n",
        "two_channel_model.fit([X_train1, X_train2], y_train, batch_size=256, epochs=2)\n",
        "print(\"Total Training time: \", time.time() - start, \"seconds\")"
      ],
      "metadata": {
        "id": "ipOLaUCtpmZ8"
      },
      "id": "ipOLaUCtpmZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_reports(XPatches, yPatches, two_channel_model, filename, 1)"
      ],
      "metadata": {
        "id": "CPa-BTaHfVpV"
      },
      "id": "CPa-BTaHfVpV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}